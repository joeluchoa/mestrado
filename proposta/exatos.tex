\section{Algoritmos Exatos}

\subsection{Programação Dinâmica}

O \rcsp\ é um problema que pode ser resolvido por algoritmos
pseudopolinomiais \cite{RH92}. Para os 
algoritmos a seguir, vamos assumir que $s = 1$ e $t = n$
% e ainda que os vértices são 
% numerados de tal forma que se $(i,j) \in A$ implica que $i < j$ (tal
% ordenação dos vértices pode ser feita em $O(n+m)$). 
Por praticidade,
vamos descrever para versão com um único recurso, mas os
algoritmos podem ser facilmente generalizados.  

\subsubsection{Algoritmo A} 

Primeiro vamos descrever a recorrência mais comumente citada na 
literatura \cite{JO66}, \cite{LA76}.
Definimos $f_j(r)$ como sendo o custo do caminho com menor 
custo de $1$ a $j$, que consome no máximo $r$ unidades de recurso, 
e assim temos a recorrência:
\begin{displaymath}
f_j(r) = \left\{
\begin{array}{lcl}
0, & &\text{se } j=1  \\ & & \text{ e } r=0,\dots,l\\
 & & \\
\infty, & & \text{se } j=2,\dots,n  \\ & & \text{ e } r=0\\
 & & \\
\min\left\{f_j(r-1), \displaystyle\min_{k|r_{kj}\leq r}\{f(r-r_{kj})+c_{kj}\}\right\}, & & \text{se } j=2,\dots,n  \\ & & \text{ e } r=1,\dots,l\\
\end{array}
\right.
\end{displaymath}

Podemos implementar um algoritmo que computa o valor de um caminho ótimo 
$OPT = f_n(l)$ em tempo $O(nml)$. 
Joksch \cite{JO66} apresentou melhoras práticas para este algoritmo, contudo
a complexidade de pior caso é não melhor que a obtida com a ideia básica.

\subsubsection{Algoritmo B} 

Podemos ainda fazer um algoritmo de programação dinâmica para resolver 
uma outra recorrência, que também 
resolve o problema em tempo pseudopolinomial \cite{RH92}.

Definimos $g_j(c)$ como sendo o custo do caminho que consome
menos recurso de $1$ a $j$, e tem custo máximo $c$. Assim, temos a recorrência:
\begin{displaymath}
g_j(c) = \left\{
\begin{array}{lcl}
0, & &\text{se } j=1 \\ & &  \text{ e } c=0,\dots,OPT\\
 & & \\
\infty, & & \text{se } j=2,\dots,n \\ & & \text{ e } c=0\\
 & & \\
\min\left\{g_j(c-1), \displaystyle\min_{k|c_{kj}\leq c}\{f(c-c_{kj})+r_{kj}\}\right\}, & & \text{se } j=2,\dots,n \\ & & \text{ e } c=1,\dots,OPT\\
\end{array}
\right.
\end{displaymath}

Observe que $OPT$ não é um valor conhecido no inicio da execução, mas ele pode ser 
expresso como $OPT = \min\{c\ |\ g_n(c) \le l\}$. Para contornar isso, devemos computar
a função $g$ iterativamente, primeiro para $c=1$ e $j=2,\dots,n$, então para $c=2$ e
$j=2,\dots,n$, e assim sucessivamente, até o primeiro valor $c'$ tal que $g_n(c') \le l$. 
Só então teremos o conhecimento do valor $OPT = c'$. A complexidade do algoritmo sugerido acima é $O(nmOPT)$.


%\subsection{K-Shortest Path}

\subsection{Relaxação Lagrangeana}

A seguir vamos apresentar o algoritmo proposto por Handler e Zang \cite{HZ80},
que se utiliza de uma relaxação de um problema de programação linear 
que modela o \rcsp.
A descrição 
que vamos fazer será para o problema com um único recurso, mas o método é 
perfeitamente aplicável ao problema com um número arbitrário de recursos.

Inicialmente, vamos apresentar uma formulação para o problema \rcsp\ usando programação linear. 
Nela teremos uma variável $x_{ij}$ para cada $(i,j)\in A$, $x_{ij} = 1$ significa
dizer que o arco $(i,j)$ está na solução e $x_{ij} = 0$
o contrário.

\begin{linearprogramwlabel}{(P)}
\mbox{minimize}
	& c(x) & = & \displaystyle\sum_{(i,j) \in A} c_{ij}x_{ij} \\
\mbox{sujeito a}
	&\displaystyle\sum_{j}{x_{ij}} - \displaystyle\sum_{k}{x_{ki}} &=& \left\{ 
        	\begin{array}{rl}
	             1, & \text{se } i = 1\\
        	     0, & \text{se } i = 2,\dots,n-1\\
         	    -1, & \text{se } i = n\\
       		\end{array} 
        	\right. & (1)\\
    	&\displaystyle\sum_{(i,j) \in A} r_{ij}x_{ij} &\le& l & (2)\\
    	& x_{ij} & \in & \{0,1\},\ (i,j) \in A & (3)

\end{linearprogramwlabel}

Na formulação acima, a restrição $(3)$ é responsável por delimitar os
possíveis valores que um componente do vetor $x$ pode assumir.
A restrição $(1)$, por sua vez, é responsável por garantir que
para um vetor $x$ ser solução viável do problema, ele deve ``conter'' um caminho
do vértice $1$ ao vértice $n$. Por fim, a restrição $(2)$ nos garante
que o conjunto de arcos induzido por um vetor $x$ viável, não excede os
recursos disponíveis.

Por conveniência de notação, nós iremos definir os seguintes termos.
Vamos definir $\cal{X}$ denotando o conjunto de vetores $x$ que satisfazem as 
equações (1) e (3), ou seja, vetores $x$ que contêm um caminho de $1$ a $n$. 
Vamos definir também a seguinte função.
$$g(x) = \displaystyle\sum_{(i,j) \in A}{r_{ij} x_{ij}} - l$$

Com as definições acima, resolver $(P)$ é equivalente a resolver o
seguinte.
$$c^* = c(x^*) = 
min \left\{\ c(x) \mid x \in {\cal{X}} \mbox{ e } g(x) \leq 0\ \right\} $$

Agora iremos aplicar a teoria da dualidade lagrangeana (como apresentada,
por exemplo, em \cite{GE74}, \cite{ML78}) como primeiro passo para 
resolver o \rcsp. 
Tendo em vista que o problema é relativamente mais simples de resolver
quando a restrição $g(x) \leq 0$ é relaxada (sem essa restrição, o
problema se reduz a caminho mínimo simples), nossa estratégia
será justamente retirar essa ``restrição complicada'' do conjunto de
restrições e a usarmos como penalidade na função objetivo (técnica essa
que é a essência da relaxação lagrangeana).

Para qualquer $u \in \mathbb{R}$, definimos a função
lagrangeana.
$$L(u) = \displaystyle\min_{x \in \cal{X}}{L(u,x)} \text{, onde }
L(u,x) = c(x) + u g(x)$$

Perceba que encontrar a solução de $L(u)$ é o problema de caminho mínimo 
no grafo original, porém
com os custos dos arcos alterados para $c_{ij} + u r_{ij}$, $(i,j) \in A$.
Temos que $L(u) \leq c^*$ para qualquer 
$u \geq 0$ (teorema fraco da dualidade), pois
$$ g(x^*) \leq 0 \Rightarrow L(u) \leq c(x^*) + u g(x^*) \leq c(x^*) = c^* \text{,}$$
o que nos permite usar $L(u)$ como um limite inferior para
o problema original. Para encontrarmos o limite inferior mais justo
possível, resolvemos o problema dual a seguir.

\begin{linearprogramwlabel}{(D)}
	& L^* = L(u^*) = \displaystyle\max_{u \geq 0}{L(u)} \hfill &&&
\end{linearprogramwlabel}

Pode ser que exista uma folga na
dualidade ({\it duality gap}), ou seja, pode ser que $L^*$ seja 
estritamente menor 
que $c^*$. Nos casos que existir essa folga, teremos que trabalhar um 
pouco mais para eliminá-la. 

Vamos, agora, descrever um método para
resolver o programa $(P)$, que usa como passo, resolver o
problema $(D)$. Por praticidade vamos denotar $x(u)$ como um caminho
que possui valor ótimo associado à função $L(u)$.

O mais natural é que, como primeiro passo, verifiquemos se o menor caminho
(não limitado, $\displaystyle\min_{x \in \cal{X}}{c(x)}$) respeita nossas restrições. Vamos chamar esse caminho de $x(0)$, pois 
$L(0) = c^* $. 
\begin{itemize}
\item Se $g(x(0)) \leq 0$, então $x(0)$ é claramente uma solução ótima de $(P)$.
\item Senão, $x(0)$ nos serve, pelo menos, como limite inferior para a solução.
\end{itemize}

Como segundo passo, devemos verificar se o caminho que consome menor quantidade de 
recursos ($\displaystyle\min_{x \in \cal{X}}{g(x)}$) respeita nossas restrições.
Vamos chamar esse caminho de $x(\infty)$, pois para valores muito grandes de $u$, o parâmetro
$c(x)$ na função $L(u)$ é ``dominado'' por $ug(x)$.
\begin{itemize}
\item Se $g(x(\infty)) > 0$, o problema não tem solução, pois o caminho que 
consume a menor quantidade de recursos consome uma quantidade maior do que 
o limite.
\item Senão, $x(\infty)$ é uma solução viável para a instância e nos serve de
limite superior para problema.
\end{itemize}

Agora com os resultados dos passos anteriores, se não temos ainda a solução
ou a prova de que a instância é inviável, temos a seguinte situação:
Dois caminhos, $x(0)$, que {\bf não é solução} e é um {\bf limite inferior} e $x(\infty)$,
que {\bf é solução viável} e é um {\bf limite superior}, $g(x(0)) > 0$ e $g(x(\infty)) \leq 0$.

Da forma como desenvolvemos a solução até então, podemos interpretar cada caminho no grafo como
uma reta no espaço $(u,L)$ da forma  $L = c(x) + u g(x)$, onde $u$ é nossa variável,
$c(x)$ é nosso termo independente (ponto onde a reta corta o eixo $L$) e 
$g(x)$ é nosso coeficiente angular. Isso nos permite
dar uma interpretação geométrica para a função $L(u)$, que será o envolope inferior do
conjunto de retas (caminhos), ou seja, $L(u)$ será um conjunto de segmentos de retas,
tal que cada ponto $(u,L)$ nesses segmentos está abaixo ou na mesma altura de qualquer 
ponto $(u,L')$ pertencente as retas associadas aos caminhos.

Com a interpretação geométrica dos caminhos, temos a informação que retas {\bf crescentes}
são associadas a caminhos {\bf não viáveis} para o nosso problema, enquando as retas 
{\bf não crescentes} são {\bf soluções viáveis}. Como estamos procurando o valor
de $L^*$ (o ponto ``mais alto'' da função $L(u)$) vamos analisar o ponto $(u',L')$ que é a intercessão
das retas associadas a $x(0)$ e $x(\infty)$. 
$$u' = (c(x(\infty)) - c(x(0))) / (g(x(0)) - g(x(\infty)))$$ 
$$L' = c(x(0)) + u' \cdot g(x(0))$$

É fato que $u' \geq 0$, pois $c(x(0))$ é mínimo, $g(x(\infty)) \leq 0$ 
e $g(x(0)) > 0$. Claramente, se existem apenas dois caminhos
o ponto $(u', L')$ é o
que maximiliza $L(u)$. O mesmo acontece quando existem vários caminhos
e $L(u') = L'$, ou seja, $L(u', x) \geq L'$ para qualquer $x \in \cal{X}$.
Um último caso ``especial'' é quando existe um caminho $x_h \in \cal{X}$
tal que $g(x_h) = 0$ e $L(u') = L(u', x_h) < L'$. Como a reta associada a $x_h$ é horizontal,
ela limita superiomente $L(u)$, e como temos o ponto $(u',L(u'))$
sobre ela, $c^* = c(x_h) = L^* = L(u')$ (neste caso não existe folga na dualidade).

Falamos, especificamente, sobre os caminhos $x(0)$ e $x(\infty)$ no parágrafo anterior,
mas o que foi dito vale no caso geral, onde temos dois caminhos disponíveis $x^+,x^- \in \cal{X}$,
tal que 
$g^+ \equiv g(x^+) > 0$, 
$g^- \equiv g(x^-) \leq 0$ e 
$c^- \equiv c(x^-) \geq c^+ \equiv c(x^+)$.
Então, temos que $u' = (c^- - c^+) / (g^+ - g^-)$ e $L' = c^+ + u'g^+$ definem o ponto
de intercessão, no espaço $(u,L)$, das retas associadas aos caminhos $x^+$ e $x^-$.
Se $L(u') = L'$ ou se $g(x(u')) = 0$, então $L(u^*) = L(u')$ é a solução do nosso
problema dual $(D)$. Caso contrário, se $g(x(u')) < 0$, então $x(u')$ é o nosso novo
caminho $x^-$, e se $g(x(u')) > 0$, então $x(u')$ é o nosso novo caminho $x^+$.
O procedimento se repete até determinarmos a solução do problema $(D)$.
Com a realização do procedimento temos disponíveis um limite inferior $LB$ ({\it lower bound})
e um limite superior $UB$ ({\it upper bound}) para o valor de $c^*$. Nós temos
que $LB = L(u^*) \leq c(x^*)$ (pelo teorema fraco da dualidade); e por definição segue que qualquer $x^-$ usado durante
o procedimento é uma solução viável, assim $UB$ é o valor do último $c^-$ ou o valor
de $c(x(u'))$ associado com o último caminho $x(u')$ {\bf se $g(x(u')) \leq 0$}.

Tendo resolvido o problema $(D)$, temos limites $LB \leq c^* \leq UB$ e uma solução
viável associada a $UB$ para o \rcsp. Quando $LB = UB$, esta solução é ótima.
Porém, quando $LB < UB$ temos um folga na dualidade. Para eliminarmos essa folga 
poderiamos considerar usar um algoritmo de $k$-ésimo menor caminho ({\it k-shortest path}) a partir do 
primeiro caminho $x$ tal que $c(x) \geq LB$ até o primeiro $x_k$ tal que $g(x_k) \leq 0$.
Como esse algoritmo precisa do conhecimento de todos os caminhos anteriores para gerar o próximo,
essa abordagem não tomaria nenhum proveito da resolução do dual.
Em contraste, determinar o $k$-ésimo menor caminho em 
relacão a função lagrangeana $L(u^*,x)$ (o que é equivalente a  
usar a função $c'$ como custo, $c'_{ij} = c_{ij} + u^* \cdot r_{ij}$, $(i,j) \in A$) 
é perfeitamente aplicável a partir da solução dual.

Vamos denotar $L_k(u^*)$, para $k = 1, 2, \dots$, como sendo o valor do $k$-ésimo menor
caminho $x_k \in \cal{X}$ em relação a função de custo $L(u^*,x)$.
Os caminhos $x_1$ e $x_2$ já são conhecidos, eles são $x^+$ e $x^-$ respectivamente, pois
se interceptam no ponto $(u^*,L(u^*))$, o que significa que possuem valor mínimo em relação
a função $L(u^*,x)$. 
Iterando sobre o $k$-ésimo caminho, $k \geq 3$, nós atualizamos $UB$ quando 
$g(x_k) \leq 0$ e $c(x_k) < UB$; e atualizamos $LB = L_k(u^*)$, pois essa 
é uma sequência não decrescente ($L_{k-1}(u^*) \leq L_k(u^*)$).
O procedimento continua até que $LB \geq UB$, e então temos a solução
do problema $(P)$, associada a $UB = c^*$, solução do \rcsp.

\begin{pseudocode}
\rcsp-LANGRANGEANA($G$, $s=1$, $t=n$, $k=1$, $r$, $l$, $c$)
    
    \hspace{-0.35cm}\quad $\rhd$ Inicialização\\
\RM{01} \x $x_0,c_0,g_0 \leftarrow L(0)$\\
\RM{02} \x se $g_0 \leq 0$\\
\RM{03} \xx então $x^*, c^* \leftarrow x_0, c_0$\\
\RM{04} \xx senão $x^+, c^+, g^+ \leftarrow x_0, c_0, g_0$\\
\RM{05} \x $x_{\infty},c_{\infty},g_{\infty} \leftarrow L(\infty)$\\
\RM{06} \x se $g_{\infty} > 0$\\
\RM{07} \xx então $x^*, c^* \leftarrow NULL, NULL$ \quad $\rhd$ Não tem solução!\\
\RM{08} \xx senão $x^-, c^-, g^- \leftarrow x_{\infty}, c_{\infty}, g_{\infty}$\\
    \quad $\rhd$ Resolvendo o Dual\\
\RM{09} \x se $x^+ \neq NIL$ e $x^- \neq NIL$ \quad$\rhd$ Se entrou nos dois ``então'' acima\\
\RM{10} \xx $LB \leftarrow 0$; $UB \leftarrow c^-$\\
\RM{11} \xx enquanto $LB < UB$ faça\\
\RM{12} \xxx $u' \leftarrow (c^- - c^+) / (g^+ - g^-)$; $L' \leftarrow c^+ + u'g^+$; $x', c', g' \leftarrow L(u')$\\
\RM{13} \xxx se $g' = 0$\\
\RM{14} \xxxx então $x^*, c^* \leftarrow x', c'$; $LB \leftarrow UB \leftarrow c'$ \\
\RM{15} \xxxxxx PÁRA o enquanto \\
\RM{16} \xxx se $L(u') = L'$ e $g' < 0$ \\
\RM{17} \xxxx então $LB \leftarrow L'$; $ UB \leftarrow \min\{UB,c'\}$; $x^- \leftarrow x'$; $u^* \leftarrow u'$ \\
\RM{18} \xxxxxx PÁRA o enquanto \\
\RM{19} \xxx se $L(u') = L'$ e $g' > 0$ \\
\RM{20} \xxxx então $LB \leftarrow L'$; $u^* \leftarrow u'$ \\
\RM{21} \xxxxxx PÁRA o enquanto \\
\RM{22} \xxx se $L(u') < L'$ e $g' > 0$ \\
\RM{23} \xxxx então $x^+, c^+, g^+ \leftarrow x', c', g'$ \\
\RM{24} \xxx se $L(u') < L'$ e $g' < 0$ \\
\RM{25} \xxxx então $x^-, c^-, g^- \leftarrow x', c', g'$; $ UB \leftarrow \min\{UB,c'\}$ \\
    \quad $\rhd$ Eliminando a folga da dualidade\\
\RM{26} \xx $x_1, x_2 \leftarrow x^+, x^-$; $k \leftarrow 2$\\
\RM{27} \xx enquanto $LB < UB$ faça\\
\RM{28} \xxx $k \leftarrow k + 1$; $x_k, c_k, g_k \leftarrow L_k(u^*)$; $LB \leftarrow L_k(u^*)$\\
\RM{29} \xxx se $g_k \leq 0$ e $c_k < UB$\\
\RM{30} \xxxx então $x^-, UB \leftarrow x_k, c_k$ \\
\RM{31} \xxx se $LB \geq UB$ \\
\RM{32} \xxxx então $x^*, c^* \leftarrow x^-, UB$ \\

\RM{33} \x devolva $x^*, c^*$

\end{pseudocode}




\begin{figure}[h!]
    \centering
        \includegraphics[scale=0.40]{figuras/exemplo_grafo.png}
    \caption{\it Grafo exemplo; os rótulos dos arcos representam $(c_{ij}, r_{ij})$. }
    \label{fig:exemplo_grafo}
\end{figure}

\begin{figure}[h!]
    \centering
        \includegraphics[scale=0.58]{figuras/exemplo_funcao_L.png}
    \caption{\it Representação geométrica do grafo da Figura \ref{fig:exemplo_grafo}.
        As retas pretas represetam os caminhos que são relevantes ao algoritmo. A ``curva'' de segmentos
    mais espessos representa o função $L(u)$. }
    \label{fig:exemplo_funcao_L}
\end{figure}


